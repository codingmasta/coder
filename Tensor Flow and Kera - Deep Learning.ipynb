{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow and Keras - Deep Learning Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Downloading tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\n",
      "     -------------------------------------- 272.8/272.8 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "     -------------------------------------- 440.7/440.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     ---------------------------------------- 14.7/14.7 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (63.4.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.1.0-cp39-cp39-win_amd64.whl (120 kB)\n",
      "     -------------------------------------- 120.3/120.3 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "     -------------------------------------- 178.2/178.2 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\rakim\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439795 sha256=54b22211d5e35857f52cf224446d1d7c4fc89dca87e5487ffe44be248c52c5d9\n",
      "  Stored in directory: c:\\users\\rakim\\appdata\\local\\pip\\cache\\wheels\\05\\94\\dc\\81042da9bced43ff430bc02043d213d9e4b210b584c39e31c1\n",
      "Successfully built jax\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, oauthlib, numpy, keras, grpcio, google-pasta, gast, astunparse, absl-py, requests-oauthlib, opt-einsum, ml-dtypes, google-auth, jax, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.1.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.23.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Install tensorflow\n",
    "\n",
    "!pip install tensorflow\n",
    "\n",
    "# Import required packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Set up train and test data for Y\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up train and test data for Y\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build an artificial neural network (ANN). Train and test the model using MNIST data. Data should have two hidden and one output layer; dense hidden layers; and neuron layers should be 32 and 16 for the first and second layers respectively. Are there any differences between the train and test data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# our first dense layer\n",
    "model.add(Dense(32, input_shape=(784,), activation=\"relu\"))\n",
    "# our second dense layer\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "# last layer is the output layer.\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 793us/step - loss: 1.7001 - accuracy: 0.4867\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 0s 746us/step - loss: 0.7050 - accuracy: 0.8190\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 0s 744us/step - loss: 0.4941 - accuracy: 0.8638\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 0s 767us/step - loss: 0.4222 - accuracy: 0.8815\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 0s 727us/step - loss: 0.3835 - accuracy: 0.8911\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 0s 733us/step - loss: 0.3580 - accuracy: 0.8974\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 0s 768us/step - loss: 0.3391 - accuracy: 0.9027\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 0s 717us/step - loss: 0.3242 - accuracy: 0.9071\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 0s 754us/step - loss: 0.3122 - accuracy: 0.9096\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 0s 752us/step - loss: 0.3016 - accuracy: 0.9137\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 0s 737us/step - loss: 0.2922 - accuracy: 0.9166\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 0s 772us/step - loss: 0.2838 - accuracy: 0.9180\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 0s 744us/step - loss: 0.2758 - accuracy: 0.9208\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 0s 714us/step - loss: 0.2683 - accuracy: 0.9227\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 0s 797us/step - loss: 0.2613 - accuracy: 0.9246\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 0s 819us/step - loss: 0.2549 - accuracy: 0.9265\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 0s 814us/step - loss: 0.2489 - accuracy: 0.9284\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 0s 948us/step - loss: 0.2430 - accuracy: 0.9301\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 0s 804us/step - loss: 0.2375 - accuracy: 0.9316\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 0s 861us/step - loss: 0.2322 - accuracy: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13eed1d1280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting verbose=1 prints out some results after each epoch\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.22757157683372498\n",
      "Test accuracy: 0.9333000183105469\n"
     ]
    }
   ],
   "source": [
    "# Determine test score and test accurary.\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved 94% accuracy in both the training and the test set. This level is lower than that of the example in the checkpoint. Since the layers include less number of neurons in this model, the model is simpler than the one of the example. This resulted in a lower performance. It seems, MNIST data requires a more complicated model than this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an ANN with 5 hidden layers and 1 output layer. All the layers should be dense. The neuron numbers for the hidden layers should be 1024, 512, 256, 128 and 64. Train this model 20 epochs and test it using the same data from the previous task and compare the results. Is there any difference? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# our first dense layer\n",
    "model.add(Dense(1024, input_shape=(784,), activation=\"relu\"))\n",
    "# our second dense layer\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "# our third dense layer\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "# our fourth dense layer\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "# our fifth dense layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "# last layer is the output layer.\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile teh model\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 1.2461 - accuracy: 0.6791\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.3652 - accuracy: 0.8985\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2745 - accuracy: 0.9212\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2313 - accuracy: 0.9328\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2015 - accuracy: 0.9419\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1781 - accuracy: 0.9491\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1609 - accuracy: 0.9542\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1449 - accuracy: 0.9580\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1326 - accuracy: 0.9619\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1213 - accuracy: 0.9652\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1111 - accuracy: 0.9675\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.1021 - accuracy: 0.9698\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0951 - accuracy: 0.9728\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0874 - accuracy: 0.9752\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0806 - accuracy: 0.9772\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0749 - accuracy: 0.9784\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0696 - accuracy: 0.9810\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0645 - accuracy: 0.9818\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0600 - accuracy: 0.9835\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0558 - accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13eed549520>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting verbose=1 prints out some results after each epoch\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0901966467499733\n",
      "Test accuracy: 0.9718999862670898\n"
     ]
    }
   ],
   "source": [
    "# Determine test score and test accurary.\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model achieved almost 99% accuracy in the training set and 98% accuracy in the test set. The model here is more complex than the model of the previous task and the model in the example. Because this model is more complex than the previous two models it achieved higher performance in the training set. However, the difference between the training score and test score widened a little bit. It may be a sign that this model is too complex and it started to overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
